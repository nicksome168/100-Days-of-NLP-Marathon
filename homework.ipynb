{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicksome168/100-Days-of-NLP-Marathon/blob/main/homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCIvz30AOj-H"
      },
      "source": [
        "# 作業 : 觀察機器翻譯 ATTENTION 內容 \n",
        "- 仔細地觀察機器翻譯 ATTENTION 結果"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usP1_X7qOv6F"
      },
      "source": [
        "# [作業目標]\n",
        "- 透過視覺化 注意力 attention 層 了解attention 的作用方式"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWGLeN9BOxEF"
      },
      "source": [
        "# [作業重點]\n",
        "- 透過視覺化 注意力 attention 層 了解attention 的作用方式\n",
        "- 原則上只要之前的訓練有跑完，這邊的程式可以執行成功最後只要觀察結果就好\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIBD2Nn-OI-1"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "import csv\n",
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQoAR8K-RyHd",
        "outputId": "90f61737-da03-49ab-fc8d-a6d7041e6fe7"
      },
      "source": [
        "# Colab 進行matplotlib繪圖時顯示繁體中文\n",
        "# 下載字體並命名taipei_sans_tc_beta.ttf，移至指定路徑\n",
        "!wget -O taipei_sans_tc_beta.ttf https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_&export=download\n",
        "!mv taipei_sans_tc_beta.ttf /usr/local/lib/python3.6/dist-packages/matplotlib//mpl-data/fonts/ttf\n",
        "\n",
        "from matplotlib.font_manager import FontProperties\n",
        "import matplotlib.pyplot as plt \n",
        "plt.style.use(\"seaborn-whitegrid\")\n",
        "import matplotlib.ticker as ticker\n",
        "# 自定義字體變數\n",
        "myfont = FontProperties(fname=r'/usr/local/lib/python3.6/dist-packages/matplotlib/mpl-data/fonts/ttf/taipei_sans_tc_beta.ttf')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-30 14:53:04--  https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_\n",
            "Resolving drive.google.com (drive.google.com)... 108.177.111.101, 108.177.111.102, 108.177.111.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|108.177.111.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0k-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/rif1tl9jetrg1or6cerogjvrf5jtsdcl/1606747950000/02847987870453524430/*/1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_ [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-11-30 14:53:05--  https://doc-0k-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/rif1tl9jetrg1or6cerogjvrf5jtsdcl/1606747950000/02847987870453524430/*/1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_\n",
            "Resolving doc-0k-9o-docs.googleusercontent.com (doc-0k-9o-docs.googleusercontent.com)... 172.217.212.132, 2607:f8b0:4001:c03::84\n",
            "Connecting to doc-0k-9o-docs.googleusercontent.com (doc-0k-9o-docs.googleusercontent.com)|172.217.212.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-font-ttf]\n",
            "Saving to: ‘taipei_sans_tc_beta.ttf’\n",
            "\n",
            "taipei_sans_tc_beta     [  <=>               ]  19.70M  68.6MB/s    in 0.3s    \n",
            "\n",
            "2020-11-30 14:53:06 (68.6 MB/s) - ‘taipei_sans_tc_beta.ttf’ saved [20659344]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrbNQhl5R2MP",
        "outputId": "ce704d72-6df2-4a07-8afa-d0557b8c2073"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFjVGqegR5U8",
        "outputId": "90e653a6-c8b6-4305-f8c6-59e538678bc8"
      },
      "source": [
        "data_dir = '/content/drive/My Drive/cupoy/attention/data/'\n",
        "lines = open(data_dir + 'cmn.txt' , encoding='utf-8').read().strip().split('\\n')\n",
        "trnslt_pairs = [[s for s in l.split('\\t')] for l in lines ]\n",
        "print (\"Sample: \" , trnslt_pairs[1000][0:2] )\n",
        "print (\"Total records:\" , len(trnslt_pairs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample:  ['I have no clue.', '我一无所知。']\n",
            "Total records: 23610\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl-KIM-nSA-H",
        "outputId": "924da190-5818-4cad-8c4e-2a0069da256b"
      },
      "source": [
        "# 下載 spacy 的英文模型 幫我們做tokenize\n",
        "model_dir =  '/content/drive/My Drive/cupoy/attention/model/'\n",
        "\n",
        "spacy_eng = spacy.load('en_core_web_sm')\n",
        "def tokenize_eng(text):\n",
        "  #清除不需要的字符\n",
        "  text = re.sub(r\"([.!?])\", r\" \\1\", text)\n",
        "  return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "TRG = Field(tokenize = tokenize_eng, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "\n",
        "\n",
        "def tokenize_cmn(text):\n",
        "  #去掉非中文字元\n",
        "  regex = re.compile(r'[^\\u4e00-\\u9fa5A-Za-z0-9]')\n",
        "  text = regex.sub(' ', text)\n",
        "\n",
        "  return [word for word in text if word.strip()]\n",
        "    \n",
        "\n",
        "SRC = Field(tokenize = tokenize_cmn, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            include_lengths = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_dataset, dev_dataset, test_dataset = TabularDataset.splits(\n",
        "    path = data_dir , format = 'csv', skip_header = True,\n",
        "    train='train.csv', validation='val.csv', test='test.csv',\n",
        "    fields=[\n",
        "        ('trg', TRG),\n",
        "        ('src', SRC)\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# 讀取之前儲存的 vocabulary\n",
        "SRC.vocab = torch.load(model_dir + 'SRC_vocab.pt')\n",
        "TRG.vocab = torch.load(model_dir + 'TRG_vocab.pt')\n",
        "\n",
        "print (\"中文語料的字元表長度: \" , len(SRC.vocab) , \", 英文的字元表長度: \" ,len(TRG.vocab))\n",
        "print (\"Sample SRC:\", test_dataset[0].src , \"TRG:\", test_dataset[0].trg)\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_dataset, dev_dataset, test_dataset), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_within_batch = True,\n",
        "     sort_key = lambda x : len(x.src),\n",
        "     device = device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "中文語料的字元表長度:  2680 , 英文的字元表長度:  4040\n",
            "Sample SRC: ['我', '昨', '天', '買', '了', '這', '台', '相', '機'] TRG: ['i', 'bought', 'this', 'camera', 'yesterday', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYoqlKcrq2Z_"
      },
      "source": [
        "# 模型主體 和前面範例程式一樣\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj3ZTHDMSGOF"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, hidden, encoder_outputs, mask):\n",
        "    # hidden bz , dec_hid_dim\n",
        "    # encoder_outputs src len, bz , enc_hid_dim x 2\n",
        "    # mask bz , src len\n",
        "    \n",
        "    batch_size = encoder_outputs.shape[1]\n",
        "    src_len = encoder_outputs.shape[0]\n",
        "\n",
        "    hidden = hidden.unsqueeze(1) \n",
        "    # hidden unsqueeze bz , 1 , dec_hid_dim\n",
        "\n",
        "    attention = torch.matmul( hidden , encoder_outputs.permute(1, 2, 0)   )\n",
        "    # attention bz, 1 , src len\n",
        "    \n",
        "    attention = attention.squeeze(1)\n",
        "    # squeeze bz , src len\n",
        "\n",
        "    attention = attention.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "    return F.softmax(attention, dim = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNzgZqHcS2CX"
      },
      "source": [
        "class RNNEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        # 雙向 ＧＲＵ encoder \n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        \n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_len):\n",
        "        \n",
        "        #src shape [src len, batch size]\n",
        "        #src_len shape [batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded shape [src len, batch size, emb dim]\n",
        "                \n",
        "        # 使用pack_padded_sequence 來壓縮序列        \n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len)\n",
        "                \n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "\n",
        "        # 使用 pad_packed_sequence 用來展開序列成原本形狀的      \n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
        "            \n",
        "            \n",
        "        #outputs shape [src len, batch size, hid dim * num directions]\n",
        "        #hidden shape [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #hidden 堆疊 [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs 是最後一層 \n",
        "        \n",
        "        #hidden [-2, :, : ] 是最後一層 forwards RNN \n",
        "        #hidden [-1, :, : ] 是最後一層 backwards RNN\n",
        "        \n",
        "        # hidden 是最後再過一層 dense layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        #outputs shape [src len, batch size, enc hid dim * 2]\n",
        "        #hidden shape [batch size, dec hid dim]\n",
        "        \n",
        "        return outputs, hidden\n",
        "\n",
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        # 單向 ＧＲＵ decoder \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs, mask):\n",
        "             \n",
        "        #input shape [batch size]\n",
        "        #hidden shape [batch size, dec hid dim]\n",
        "        #encoder_outputs shape [src len, batch size, enc hid dim * 2]\n",
        "        #mask shape [batch size, src len]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input shape [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded shape [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs, mask)\n",
        "                \n",
        "        #a shape [batch size, src len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "        \n",
        "        #a shape [batch size, 1, src len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #encoder_outputs shape [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        \n",
        "        #weighted shape [batch size, 1, enc hid dim * 2]\n",
        "        \n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        \n",
        "        #weighted shape [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        \n",
        "        #rnn_input shape [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "            \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output shape [seq len, batch size, dec hid dim * n directions]\n",
        "        #hidden shape [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output shape [1, batch size, dec hid dim]\n",
        "        #hidden shape [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        \n",
        "        #prediction shape [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)\n",
        "\n",
        "class Seq2SeqATTN(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def create_mask(self, src):\n",
        "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
        "        return mask\n",
        "        \n",
        "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #src_len = [batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "                    \n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
        "                \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        mask = self.create_mask(src)\n",
        "\n",
        "        #mask = [batch size, src len]\n",
        "                \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden state, all encoder hidden states \n",
        "            #  and mask\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "            \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlITuy6WS47j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50mOv4N-S8LJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW2KIxhxrMGf"
      },
      "source": [
        "# 建立模型和重要參數 請保持和前面訓練時一樣"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybIY0kKGS_gI",
        "outputId": "f3c7921e-ee12-4039-8dce-a72e64d7a4d6"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 256 # 注意 encoder hidden layer 設定 必須為 dec 的一半 \n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "LEARNING_RATE = 0.002\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = RNNEncoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = RNNDecoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "model = Seq2SeqATTN(enc, dec, SRC_PAD_IDX, device).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
        "\n",
        "\n",
        "def initial_mdl_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(initial_mdl_weights)\n",
        "print (\"模型全部參數量: {:10,d} \".format(sum(p.numel() for p in model.parameters())))\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "模型全部參數量:  9,916,872 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2SeqATTN(\n",
              "  (encoder): RNNEncoder(\n",
              "    (embedding): Embedding(2680, 256)\n",
              "    (rnn): GRU(256, 256, bidirectional=True)\n",
              "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): RNNDecoder(\n",
              "    (attention): Attention()\n",
              "    (embedding): Embedding(4040, 256)\n",
              "    (rnn): GRU(768, 512)\n",
              "    (fc_out): Linear(in_features=1280, out_features=4040, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOpjxQJmTDYU"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src, src_len = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, src_len.cpu(), trg, 0) #turn off teacher forcing\n",
        "            \n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ukg9t_iOTHlG",
        "outputId": "b2c6b18c-5243-4b66-fd4f-8ab83d321c3e"
      },
      "source": [
        "model_dir =  '/content/drive/My Drive/cupoy/attention/model/'\n",
        "model.load_state_dict(torch.load(model_dir + 'best-model.pt'))\n",
        "#model.load_state_dict(torch.load(model_dir + 'model-7.pt'))\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 3.586 | Test PPL:  36.086 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-caE1Y1TL5p"
      },
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
        "\n",
        "    model.eval()\n",
        "        \n",
        "    #if isinstance(sentence, str):\n",
        "    #    nlp = spacy_en = spacy.load('en_core_web_sm')\n",
        "    #    tokens = [token.text.lower() for token in spacy_en(sentence)]\n",
        "    #else:\n",
        "    #    tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [token.lower() for token in sentence]\n",
        "        \n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    \n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "    src_len = torch.LongTensor([len(src_indexes)]).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(src_tensor, src_len.cpu())\n",
        "\n",
        "    mask = model.create_mask(src_tensor)\n",
        "        \n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\n",
        "    \n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n",
        "\n",
        "        attentions[i] = attention\n",
        "            \n",
        "        pred_token = output.argmax(1).item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jhh_5_SYYLT"
      },
      "source": [
        "def display_attention(sentence, translation, attention):\n",
        "    \n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(111)\n",
        "    \n",
        "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
        "    \n",
        "    cax = ax.matshow(attention, cmap='bone')\n",
        "   \n",
        "    #fontdict = {\"fontproperties\": zhfont}\n",
        "    \n",
        "    #ax.set_xticks(range(max(max_len_tar, len(predicted_seq))))\n",
        "    #ax.set_xlim(-0.5, max_len_tar -1.5)\n",
        "    \n",
        "    #ax.set_yticks(range(len(sentence) + 2))\n",
        "    #ax.set_xticklabels([subword_encoder_zh.decode([i]) for i in predicted_seq \n",
        "    #                    if i < subword_encoder_zh.vocab_size], \n",
        "    #                   fontdict=fontdict, fontsize=18)\n",
        "    \n",
        "    #plt.rcParams[\"font.family\"]=\"sans-serif\"\n",
        "    #plt.rcParams['font.sans-serif']=['STSong'] #用来正常显示中文标签\n",
        "    \n",
        "    ax.tick_params(labelsize=15)\n",
        "    ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
        "                       rotation=45 , fontproperties=myfont) #, fontdict=fontdict)\n",
        "    ax.set_yticklabels(['']+translation, fontproperties=myfont) # , fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4n7915Mrcs1"
      },
      "source": [
        "# 作業重點\n",
        "## 請選擇一個好的翻譯結果\n",
        "## 將其 ATTENTION 視覺化 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRYXjqgvYb-E",
        "outputId": "3532080c-035a-48f1-c89d-107740135895"
      },
      "source": [
        "# 請在這邊自行調整 sample index \n",
        "# 觀察不同句子的 ATTENTION 結果\n",
        "example_idx =499\n",
        "\n",
        "src = vars(train_dataset.examples[example_idx])['src']\n",
        "trg = vars(train_dataset.examples[example_idx])['trg']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')\n",
        "\n",
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['她', '是', '我', '女', '儿']\n",
            "trg = ['she', \"'s\", 'my', 'daughter', '.']\n",
            "predicted trg = ['she', \"'s\", 'my', 'daughter', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cdlptGJrsfv"
      },
      "source": [
        "# 請觀察翻譯文 和被翻譯文的語意對應"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "H8f8csPSYfkU",
        "outputId": "61910059-d71a-4960-fa1a-b83db6cb6a39"
      },
      "source": [
        "print (\"\".join(src ))\n",
        "display_attention(src, translation, attention)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "她是我女儿 05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ake_IJd2YiG3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}